## **3blue1brown**

什么是向量？它有长度和方向，可以理解为有序的数字列表

不同的基向量可以理解为不同的坐标系
共线的基向量张成的空间是直线
什么是基？张成该空间的线性无关的向量集

什么是线性变换？直线经过变换仍为直线，原点不动，网络线保持平行且等距分布

线性代数可以求解特定的线性方程组

矩阵乘法是两个变换相继作用

什么都不做的变换称为恒等变换

无法将一条直线 “解压缩” 为一个平面？因为这个要求一条直线被映射为多个向量，而函数只能将一个输入变换为一个输出

什么是矩阵的秩？经过该矩阵变换后的空间的维度

矩阵A的列空间即基向量 $\vec{v}$ 讲过矩阵 $A$ 变换后的向量 $A\vec{v}$ 构成的空间

向量的叉积即两个向量构成的平行四边形的面积，也是 $det([\vec{v}, \vec{w}])$

特征向量
若向量 $\vec{v}$ 是 $A$ 的特征向量，则 $v$ 会在变换 $A\vec{v}$ 后停留在它张成的子空间里
$$
A\vec{v} = \lambda \vec{v} \\
A\vec{v}-\lambda \vec{v} = \vec{0} \\
(A-\lambda)\vec{v} = \vec{0} \\
det((A-\lambda)\vec{v}) = 0  \quad 得到 \lambda 带回上式计算得 \vec{v}\\
$$
特征向量方向固定，经过变换 $A$ 后被拉伸为原来的 $\lambda$ 倍

基向量可以是特征向量，例如：对角矩阵。也可以将特征向量转换为基向量，即寻找特征基

线性的严格定义
$$
L(\vec{v} + \vec{w}) = L(\vec{v}) + L(\vec{w}) \\
L(c\vec{v}) = cL(\vec{v})
$$

公理不是自然法则而是媒介
普适的代价是抽象

***

## **应用**

大型稀疏矩阵在生活中的应用非常广泛，例如：

搜索引擎：搜索引擎索引的网页可以看作是一个巨大的稀疏矩阵，其中每一行代表一个网页，每一列代表一个关键词，非零元素表示该网页包含该关键词。

**推荐系统**：推荐系统利用用户对商品的评分数据构建用户评分矩阵，该矩阵通常是一个大型稀疏矩阵，其中每一行代表一个用户，每一列代表一个商品，非零元素表示该用户对该商品的评分。

图像处理：图像处理中的滤波器通常是一个稀疏矩阵，其中非零元素表示滤波器的权值。在图像压缩中，离散余弦变换（DCT）也会生成一个大型稀疏矩阵。

自然语言处理：在自然语言处理中，通常需要构建文本的共现矩阵，该矩阵也是一个大型稀疏矩阵，其中每一行代表一个单词，每一列代表一个文本，非零元素表示该单词在该文本中出现。

网络分析：社交网络和通信网络都可以表示为一个稀疏矩阵，其中每一行代表一个节点，每一列代表一个节点或边，非零元素表示两个节点之间存在连接。






### 参考资料

https://linalg.apachecn.org/