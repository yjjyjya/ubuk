# 参考资料

https://github.com/zhouzhongmi/ML_Notes/blob/master/README.md



## 如何将算法进行封装？

``` python
class algorithm_name(object):

    def __init__(self, 模型参数):
        self.bins = bins
        self.thresh = thresh
        self.thresh_value = None
        self.hist_bins = {}
        self.pca = None
        self.pca_mins = None

    def fit_transform(self, X):
        # 模型拟合并计算结果
        return (hbos >= self.thresh_value).astype(int)

    def transform(self, X):
        # 仅计算结果，将模型应用到传入的参数 X 上
        return (hbos >= self.thresh_value).astype(int)
```

--- 

## 降维

### 奇异值分解

$A$ 是一个 $m \times n$ 的实矩阵，它可以分解为三个实矩阵的乘积形式

$$
A = U\Sigma V^{T}
$$

其中

$$
UU^T=I, VV^T=I\\
\Sigma=diag(\sigma_1,\sigma_2,...,\sigma_p)
$$

$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_p \geq 0$ 且 $p=min(m,n)$

奇异值分解定理：任意 $m \times n$ 的实矩阵都可以进行奇异值分解

- 求 $\Sigma, V$  
    由于 $A$ 是 $m \times n$ 阶的矩阵，所以 $A^TA$ 是一个 $n \times n$ 实对称矩阵，故可以对其做正交分解使得：

    $$
    V^T(A^TA)V = \Lambda
    $$  

    其中，$V$ 为 n 阶正交实矩阵，$\Lambda$ 为 $n$ 阶 **对角阵**，其对角线元素由 **$A^TA$ 的特征值** 构成，且这些 **特征值非负**

    > ![NOTE|style:flat]
    > 假设 $\lambda$ 是 $A^TA$ 的一个特征值，$x$ 是对应的特征向量，那么  
    > 
    > $$
    > ||Ax||^2=x^TA^TAx=\lambda x^Tx=\lambda ||x||^2
    > $$  
    > 
    > 所以  
    > 
    > $$
    > \lambda=\frac{||Ax||^2}{||x||^2}>0
    > $$

    我们可以通过调整正交矩阵 $V$ 的排列顺序使得对应的特征值形成降序排列  

    $$
    \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_p \geq 0
    $$

    接着计算奇异值  

    $$
    \sigma_j = \sqrt{\lambda_j}, j=1,2,...,q
    $$

    假设 $A$ 的秩为 $r$，则 $A^TA$ 的秩也为 $r$，所以有  

    $$
    \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_r > 0, \\ \lambda_{r+1}=\lambda_{r+2}=\cdots=\lambda_p=0
    $$  

    相应的，我们令  

    $$
    V_1=[v_1,...,v_r], V_2=[v_{r+1},...,v_p]
    $$  

    则   

    $$
    V=[V_1\ V_2]
    $$  

    同样地，我们令  

    $$
    \Sigma_1=diag(\sigma_1,...,\sigma_r)
    $$  

    对其余部分填充 $0$，使得   

    $$
    \Sigma = \begin{bmatrix}
    \Sigma_1 & 0 \\ 
    0 & 0
    \end{bmatrix}
    $$

- 求 $U$

    构造 $m$ 阶正交实对称矩阵 $U$，我们令  

    $$
    u_j = \frac{1}{\sigma_j}A v_j, j=1,2,...,r
    $$

    令  

    $$
    U_1 = [u_1,...,u_r]
    $$

    则  

    $$
    AV_1 = U_1\Sigma_1
    $$

    接下来，再为 $U_1$ 扩充 $m-r$ 个标准正交向量，令 $[u_{r+1},...,u_m]$ 为 $N(A^T)$ 的一组正交基，并令  

    $$
    U_2 = [u_{r+1},...,u_m] \\
    U = [U_1\ U_2]
    $$

    所以  

    $$
    \begin{align}
        U \Sigma V^T = \,& \begin{bmatrix}
        U_1 & U_2
        \end{bmatrix}
        \begin{bmatrix}
        \Sigma_1 & 0 \\
        0 & 0
        \end{bmatrix}
        \begin{bmatrix}
        V_1^T \\ 
        V_2^T 
        \end{bmatrix} \\
        = \,& U_1 \Sigma_1 V_1^T \\
        = \,& AV_1V_1^T \\
        = \,& A
    \end{align}
    $$

    但是 $r<p$，其实不需要对 $U_1$ 以及 $V_1$ 进行扩充  

代码示例：
``` python
import numpy as np
import matplotlib.pyplot as plt


plt.figure(figsize=(18,4))
im = plt.imread('./.../...jpg')
ks = [800,500,200,100,50,10]    #分别截取不同的 k
for idx, k in enumerate(ks):
    svd_image = []
    for ch in range(3):    #注意,有 RGB 三个维度,每个维度对应一个矩阵做 SVD 分解
        im_ch = im[:,:, ch]
        U,D,VT = np.linalg.svd(im_ch)
        imx = np.matmul(np.matmul(U[:,:k], np.diag(D[:k])), VT[:k,:])
        # 将像素值约束到合理范围
        imx = np.where(imx<0, 0, imx)
        imx = np.where(imx>255, 255, imx)
        svd_image.append(imx.astype('uint8'))
    img = np.stack((svd_image[0], svd_image[1], svd_image[2]), 2)    #重构图片矩阵
    plt.subplot(1, len(ks), idx+1)
    plt.imshow(img)
    plt.axis('off')
    plt.title("k="+str(k))
```


### PCA

对原始坐标系进行变换  
使得原始数据在 **新的坐标轴** 上的第一个坐标轴上的投影 **尽可能分散开**，称为 **第一主成分**，第二个坐标轴次之，称为第二主成分  

存在的问题：
- 如何计算投影坐标？  
    将样本点 $x$ 看作一个向量，起点为原点，终点为样本点 $x$  
    设一坐标轴的方向向量为 $w$，将其移动至与 $x$ 相同起点的位置，夹角为 $\theta$
    $$
    ||x|| \cos\theta = \frac{||x||\cdot||w|| \cos\theta}{||w||} = \frac{x^{T}w}{||w||}
    $$
- 如何衡量分散程度？  
    利用方差，方差越大，说明数据越分散  
    $$
    S(X,w) = \sum_{i=1}^{m}(x_i^Tw-\bar{x}^Tw)^2
    $$
    $\bar{x}$ 为 $x_{i} \in R^{n}$ 的均值  
    通常在此之前，我们会先进行去中心化操作  
    $$
    X = X - \bar{X}
    $$
    方差就可以简化为  
    $$
    \begin{align}
    S(X,w) = \,& \sum_{i=1}^{m}(x_i^Tw)^2 \\
    = \,& w^{T}X^{T}Xw
    \end{align}
    $$
    故欲求的最优$w^*$便是  
    $$
    w^* = arg\max_{w}w^TX^TXw
    $$  

### LDA

Linear discriminant analysis，线性判别分析，是监督式的降维方法  
核心思想：找一条直线，使得类内距尽可能小，类间距尽可能大  

- 二分类情况  
    两个类别对应的样本集合分别为 $X_0,X_1$，中心点分别为 $\mu_0,\mu_1$，投影坐标表示为 $w$，那么：   

    （1）类间方差可以表示为 $\left|\left|w^T\mu_0-w^T\mu_1\right|\right|_2^2=w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw$  
    投影后，类中心点的方差

    （2）类内方差可以表示为 $w^T(\Sigma_0+\Sigma_1)w$，这里：   

    $$
    \Sigma_0=\sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T \\
    \Sigma_1=\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T
    $$  

    为了方便表示，我们定义：   

    $$
    S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T\\
    S_w = \Sigma_0+\Sigma_1
    $$  

    所以，我们的优化目标便有了：  

    $$
    arg\max_w \frac{w^TS_bw}{w^TS_ww}
    $$

- 多分类情况  
    作为二分类的推广，对于有 $j=1,2,..,k$ 种类别，定义每种类别对应的样本集为 $X_j$，样本中心点为 $\mu_j$，样本量为 $N_j$，整体中心点为 $\mu$，我们可以定义  

    $$
    S_b=\sum_{j=1}^kN_j(\mu_j-\mu)(\mu_j-\mu)^T\\
    S_w=\sum_{j=1}^k\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T
    $$

    转换成了各类别与整个样本中心点的计算问题

### MDS

Multiple Dimensional Scaling，多维缩放算法  
基本思想：数据降维压缩后，不改变样本间彼此的欧氏距离  

设原始空间样本集为 $X\in R^{m\times n}$  
样本张成的距离矩阵为 $D\in R^{m\times m}$，$D_{ij}$ 表示样本 $x_i$ 与样本 $x_j$ 之间的距离  
降维后的样本集为 $Z\in R^{m\times d}(d<<n)$  
目标是让如下关系式成立：  

$$
\left|\left| z_i-z_j \right|\right| = D_{ij}
$$

为了方便处理，我们令 $B=Z^TZ$(即 $B_{ij}=z_i^Tz_j$)，那么上面的关系式可以等价为如下关系：  

$$
\begin{align}
D_{ij}^2 = \,& \left|\left|z_i\right|\right|^2+\left|\left|z_j\right|\right|^2-2z_i^Tz_j \\
= \,& B_{ii}+B_{jj}-2B_{ij}
\end{align}
$$

所以，我们的目标便是由 $D$ 矩阵推导出 $B$ 矩阵，然后再对 $B$矩阵做正交分解得到 $Z$  
为了便于处理，我们假设 $Z$ 被去中心化了，即 $\sum_{i=1}^mz_i=0$，那么有：  

$$
\sum_{i=1}^mB_{ij}=\sum_{j=1}^mB_{ij}=0
$$

进一步地，有如下关系成立：  

$$
\sum_{i=1}^mD_{ij}^2=tr(B)+mB_{jj}(关系1)\\
\sum_{j=1}^mD_{ij}^2=tr(B)+mB_{ii}(关系2)\\
\sum_{i=1}^m\sum_{j=1}^mD_{ij}^2=2mtr(B)(关系3)\\
$$

其中，$tr(B)=\sum_{i=1}^mz_i^Tz_i$，通过关系1,2,3，我们可以得到$B$关于$D$表达式，为了进一步简化，我们可以令：   

$$
D_{i\cdot}^2=\frac{1}{m}\sum_{j=1}^mD_{ij}^2\\
D_{\cdot j}^2=\frac{1}{m}\sum_{i=1}^mD_{ij}^2\\
D_{\cdot \cdot}^2=\frac{1}{m^2}\sum_{i=1}^m\sum_{j=1}^mD_{ij}^2
$$  

最后：   


$$
B_{ij}=-\frac{1}{2}(D_{ij}^2-D_{i\cdot}^2-D_{\cdot j}^2+D_{\cdot \cdot}^2)
$$  

我们对 $B$ 做特征分解 $B=V\Lambda V^T$，其中 $\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_n),\lambda_1\geq\lambda_2\geq\cdots\geq\lambda_n$，而： 

$$
Z = V\Lambda^{1/2}
$$

与 PCA，LDA 类似地，我们通常只取 $\Lambda$ 中特征值较大的前几个，对应的 $V$ 也只取前几列  

### 流形学习

Isomap，等度量映射  
MDS 直接采用了 **欧氏距离** 构建了原始高维空间的距离，这在流形结构的数据中会出现问题

- Isomap的操作：  
    （1）对样本中的每个点，保留与它最近的 $k$ 个点（或者 $\varepsilon$ 领域半径内的点）的欧氏距离，而其他点的距离设置为无穷大  
    （2）采用 Dijkstra 算法或者 Floyd 算法计算所有样本中 **任意两点间的最短距离**，并更新原始距离矩阵 $D$  
    之后的操作同 MDS 一样  

- 问题  
    Isomap 会受到最近邻 $k$ 或者近邻半径 $\varepsilon$ 的影响  
    （1）过小，可能会存在“断路”的情况，图中某些区域可能与其他区域不存在连接，直观来看就是距离矩阵通过floyd算法更新后还存在`np.inf`  
    （2）过大，则会存在“短路”的情况，使得距离失真，比如本节最上图中的黑线距离会取代红线距离  
    在实际使用时通过后续任务的表现（分类/回归/...任务的具体表现）来选取  






---

## 异常检测

核心是找到 “不合群” 的点，将 “不合群” 转化为量化指标

### HBOS 算法

Histogram-based Outlier Score，基于直方图的异常点检测算法
原理：将数据各维度的直方图概率相加，整体概率值越小的样本越有可能是异常点  

衡量指标：  

$$
HBOS(p) = \sum_{i=1}^{d} log(\frac{1}{\text{hist}_{i}(p)})
$$

$\text{hist}_{i}^{p}$ 表示样本 p 在第 i 维特征上的 **直方图概率**（怎么求？？？）  

示例代码：  
``` python
# 计算 HBOS 异常值
hbos = np.zeros_like(hist_X[:,0])
for i in range(0, hist_X.shape[1]):
    hbos += np.log(1.0/(hist_X[:,i]+1e-7))
```

如果存在高度相关的因子，HBOS 指标值无法去除相关性  
一个想法是先利用 PCA 处理数据，去除相关性  

### IForest 算法

Isolation Forest，孤立森林  
主要思想：对样本空间，每次随机选取某一维度，在该维度上根据最大最小值随机选择一个阈值对样本进行划分，重复上述操作，直到每个样本被单独分离出来。越早被分离出来的样本越有可能是异常点  

### KNN 检测

借助最近邻分类的思想，若某个样本与离它最近的k个样本距离之和越大，则它越有可能是异常点  
对于大量数据，KNN很吃力，平均时间复杂度过高

### LOF 算法

Local Ourlier Factor，局部异常因子  
解决了 KNN 中基于绝对距离而忽略了特殊数据分布的问题  
局部可达密度  
局部异常因子  